---
layout: post
title: "Hello and Welcome"
date: "2025-06-18 19:55 +0100"
filename: 2025-06-18-Welcome.md
---
### Introduction

Welcome to me continuing my journey through the fascinating and ever-growing world of Artificial Intelligence. My name is Tom Jago, and I am thrilled to share my passion for exploring these cutting-edge technologies. This Portfolio will serve as documentation for my growing expertise and passion for AI, documenting my achievements and sharing insights from my experiments.

---

### My Initial Experiment: Running DeepSeek Models Locally

Recently, I embarked on an exciting project to run **DeepSeek-r1:8b** and **DeepSeek-r1:14b** models locally. This endeavor was made possible through
the use of a **Linux Virtual Machine** hosted on my Windows system via **Windows Subsystem for Linux (WSL)**.

#### Why WSL?

Choosing WSL as the medium for this experiment was strategic. It allows me to leverage the full power of Linux-based tools and environments directly
from my Windows operating system, bridging the gap between two worlds seamlessly. This setup has been instrumental in facilitating my exploration of
AI without compromising on performance or flexibility.

#### The Setup Process

1. **Virtual Machine Installation**: I set up a virtual machine using Windows PowerShell to encapsulate my Linux environment. This isolated space
ensures that all AI-related tools and configurations are neatly contained, avoiding conflicts with my primary system.

2. **Installed oLLama**: After learning about oLLama through a series of YouTube videos I decided to finally run a LLM model locally. oLLama reignited my interest into doing this as it was something that I was previously intrigued by.

3. **DeepSeek Models**: Acquiring and configuring the DeepSeek models was a task filled with curiosity and learning. These models, available in as little as 5 billion parameters, all the way up to a massive 671 billion. The choice for me was simple, due to the size of these models and the amount of parameters on the larger models, I started off with running **DeepSeek-r1:8b**, I found that it ended up using about 8 gbs of VRAM and considering that my personal computer at home as a GPU available with up to 16GBs of VRAM I then decided to experiment further. I then ran **DeepSeek-r1:14b** locally as this seemed like the next logical model available from **DeepSeek** as I felt that anything larger would be far too big to be able to run on my computer.

---

### Portfolio Goals

- **Documentation**: To keep track of all projects, experiments, and learnings in one centralized location.
- **Knowledge Sharing**: To serve as a resource for others interested in AI.
- **Personal Growth**: To reflect on my journey, celebrate milestones, and continuously improve my skills.
- **Implementation of Degree work** : To implement and discuss all work that I have previously done throughout my academic career.

<!-- excerpt-end -->
---
